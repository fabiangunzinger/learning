{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal:\n",
    "- Read through all sections in hundredpage ml book\n",
    "- Read through relevant sections in ISL\n",
    "- Read through relevant sections in applied predictive modelling\n",
    "- Read through relevant sections in python datascience handbook\n",
    "- Incorporate additional relevant sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we estimate f?**\n",
    "\n",
    "- Purpose of ml is often to infer a function f that describes the relationship between target and features.\n",
    "\n",
    "- Can estimate f for (1) prediction or (2) inference or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we estimate f?**\n",
    "\n",
    "- 3 basic approaches: parametric (assume shape of f and estimate coefficients), non-parametric (also estimate shape of f), semi-parametric.\n",
    "\n",
    "- Accuracy depends on (1) irreducible error (variance of error term) and (2) reducible error (appropriateness of our model and its assumptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ingredients to statistical learning**\n",
    "\n",
    "- Specify aim\n",
    "\n",
    "- Gather and pre-process data\n",
    "\n",
    "- Select a learning algorithm\n",
    "\n",
    "- Apply learning algorithm to data to build (and train) a model\n",
    "\n",
    "- Assess model performance (by testing) and tune model\n",
    "\n",
    "- Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of learning**\n",
    "\n",
    "- Supervised (labelled examples)\n",
    "\n",
    "- Unsupervised (unlabelled examples)\n",
    "\n",
    "- Semi-supervised (labelled and unlabelled examples)\n",
    "\n",
    "- Reinforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The trade-off between prediction accuracy and model interpretability**\n",
    "\n",
    "- Linear models vs non-linear models (hard to interpret models often predict more accurately)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised vs. unsupervised learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression vs classification problems**\n",
    "\n",
    "- Classification assigns categorical labels, regression real-valued labels to unlabelled examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters vs parameters**\n",
    "\n",
    "- Hyperparameters determine how the algorithm works and are set by the researcher.\n",
    "\n",
    "- Parameters determine the shape of the model and are estimated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model-based vs instance-based learning**\n",
    "\n",
    "- Model-based algorithms estimate and then use parameters to make predictions (i.e. can discard training data once you have estimate), instance-based algorithms (e.g. KNN) use the entire training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep vs shallow learning**\n",
    "\n",
    "- Shallow learning algorithms learn parameters directly from features, deep learning algorithms (deep neural network) learn them from the output of preceeding layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating fit\n",
    "\n",
    "- RSE \n",
    "\n",
    "- R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual standard error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The RSE is 3.25, which implies that our estimates deviate about 3.25 from the actual values (this would be true even if we knew the population parameters, as the RSE is an estimate of the error standard deviation). Given the average value of sales, the percentage error is about 12 percent. Whether this is a lot or not depends on the application. \n",
    "\n",
    "- Becaue the RSE is an absolute measure of lack of fit, expressed in units of y, it's not always easy to interpret whether a given RSE is small or large.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$\n",
    "\n",
    "- $R^2$, which is a relative measure of lack of fit, and measures the percentage of variance in y that the model can explain (and is thus always between 0 and 1). In the simple linear regression setting, $R^2 = Cor(X, Y)^2$.\n",
    "\n",
    "- A low $R^2$ can mean that the true relationship is non-linear or that the error variance is very high or both. What constitutes \"low\" depends on the application.\n",
    "\n",
    "- In the model above, more than 90 percent of the variation is explained by the set of explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is at least one of the predictors useful in explaining y?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To test whether at least one of the predictors is useful in predicting the response, we can look at the reported F statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(570.2707036590942, 1.575227256092437e-96)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.fvalue, res.f_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To test whether a subset of parameters is useful, we can run our own F-test. To manually test for all parameters, we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'statsmodels.stats.contrast.ContrastResults'>\n",
       "<F test: F=array([[570.27070366]]), p=1.5752272560925203e-96, df_denom=196, df_num=3>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.identity(len(res.params))[1:]\n",
    "res.f_test(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which is equivalent to the statistic provided in the output. To test the (joint) usefulness of radio and newspaper, we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'statsmodels.stats.contrast.ContrastResults'>\n",
       "<F test: F=array([[272.04067681]]), p=2.829486915701129e-57, df_denom=196, df_num=2>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.identity(len(res.params))[[2, 3]]\n",
    "res.f_test(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember: the F statistic is valuable because irrespective of $p$, there is only a 5 percent change that the p-value is below 0.05. In contrast, individual predictors each have that probability, so for a large number of predictors, it's very likely that we observe significant ones solely due to chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are all of the predictors or only a subset useful in explaining y?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic and advanced practice in hundreppage book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the quality of fit\n",
    "\n",
    "- MSE for regressions.\n",
    "\n",
    "- Error rate for classification.\n",
    "\n",
    "- Beware of overfitting! Maximise mse or error rate for testing data, not for training data (overfitting).\n",
    "\n",
    "- Overfitting definition: situation where a simpler model with a worse training score would have achieved a better testing score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias-variance trade-off\n",
    "\n",
    "- MSE comprises (1) squared bias of estimate, (2) variance of estimate, and (3) variance of error. We want to minimise 1 and 2 (3 is fixed).\n",
    "\n",
    "- Relationship between MSE and model complexity is U-shaped, because variance increases and bias decreases with complexity. We want to find optimal balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification setting\n",
    "\n",
    "- Bayesian classifier as unattainable benchmark (we don't know P(y|x)).\n",
    "\n",
    "- KNN one approach to estimate P(y|x), then uses bayesian classifier.\n",
    "\n",
    "- Intuition as for MSE error: testing error rate is U-shaped in K (higher K means more flexibel model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix\n",
    "\n",
    "- Can be used to calculate precision, recall, specificity, accuracy, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- True positive: there was an event and we predicted one.\n",
    "- True negative: there was no event and we didn't predict one.\n",
    "- False positive: there was no event but we predicted one (i.e. we predicted 1 instead of 0).\n",
    "- False negative: there was an event but we didn't preditc one (i.e. we predicted 0 instead of 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curves and AUC\n",
    "\n",
    "- Plots the trade-off between the false positive rate (x-axis) and the true positive rate (y-axis) - the trade-off between the false alarm rate and the hit rate.\n",
    "\n",
    "- True positive rate $= \\frac{True Positives}{True Positives + False Negatives} = \\frac{True Positives}{All Events}$.\n",
    "\n",
    "- The true positive rate is also called sensitivity. And we can think of it as the hit rate: the propostion of events that we correctly classified as such.\n",
    "\n",
    "- False positive rate $= \\frac{False Positives}{False Positives + True Negatives} = \\frac{False Positives}{All NonEvents}$.\n",
    "\n",
    "- The false positive rate is also called the false alarm rate, the proportion of cases incorrectly classified as an event among all cases that are not an event. It is also referred to as inverterd specificity, where specificity is $= \\frac{True Negatives}{False Positives + True Negatives}$.\n",
    "\n",
    "- The ROC is useful because it directly shows false/true negatives (on the x-axis) and false/true positives (on the y-axis) for different thresholds and thus helps choose the best threshold, and because the AUC can be read as an overall model summary, and thus allows us to compare different models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision-recall curves\n",
    "\n",
    "- Precision and recall originate in the field of information retrieval (e.g. getting documents from a query) but are also useful in machine learning. \n",
    "\n",
    "- Precision $= \\frac{True Positives}{True Positives + False Positives} = \\frac{True Positives}{All Positives}$\n",
    "\n",
    "- Recall $= \\frac{True Positives}{True Positives + False Negatives} = \\frac{True Positives}{All Events}$\n",
    "\n",
    "- In the context of document retriavel, precision is the useful documents as a proportion of all retrieved documents, recall the retrieved useful documents as a proportion of all available useful documents.\n",
    "\n",
    "- We can think of precision as positive predictive power (how good is the model at predicting the positive class?), while recall is the same as sensitivity -- the proportion of all events that were successfully predicted.\n",
    "\n",
    "- The precision-recall curve is particularly useful when we have much more no-event than event cases. In this case, we're often not interested much in correctly predicting no-events but focused on correctly predicting events. Because neither precision nor recall use true negatives in their calculations, they are well suited to this context ([paper](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432)).\n",
    "\n",
    "- The precision recall curve plots precision (y-axis) and recall (x-axis). An unskilled model is a horizontal line at hight equal to the proportion of events in the sample. We can use ROC to compare models as different thresholds, or the F-score (the harmonic mean between precision and recall) at a specific threshold.\n",
    "\n",
    "- Use precision-recall curves if classes are imbalanced, in which case ROC can be misleading (see example in last section [here](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)).\n",
    "\n",
    "- Shape of curve: recall increases monotonically as we lower the threshold (move from left to right in the graph) because we'll find more and more true positives (\"putting ones from the false negative to the true positive bucket\", which increases the numerator but leaves the denominator unchanged), but precision needn't fall monotonically, because we also increase false positives (both the numerator and the denominator increase as we lower the threshold, and the movement of recall depends on which increases faster, which depends on the sequence of ordered predicted events). See [here](https://stats.stackexchange.com/a/183506) for a useful example.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One way to get a sense of how non-linear the problem is, is to compare the MSE of a simple linear model and a more complex model. If the two are very close, then assuming linearity and using a simple model is preferrable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/Logistic_function)\n",
    "\n",
    "- [Roc](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habits",
   "language": "python",
   "name": "habits"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
